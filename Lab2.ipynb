{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeelamTharunKumar/Google_Colab/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir-q1uzUrruy",
        "outputId": "44c20d6e-bb39-4de6-d00e-9613bc475112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFJ5aiJ9tMPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcDTimmKgVcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c3768a-d0a0-4e3e-c01c-44be70253795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample training dataset\n",
        "tagged_sentences = [\n",
        "    [('the', 'DET'), ('dog', 'NOUN'), ('barks', 'VERB')],\n",
        "    [('a', 'DET'), ('cat', 'NOUN'), ('meows', 'VERB')],\n",
        "    [('the', 'DET'), ('cat', 'NOUN'), ('sleeps', 'VERB')],\n",
        "    [('a', 'DET'), ('dog', 'NOUN'), ('runs', 'VERB')]\n",
        "]\n",
        "\n",
        "# Extract vocabulary and tagset\n",
        "vocab = set(word for sentence in tagged_sentences for word, _ in sentence)\n",
        "tagset = set(tag for sentence in tagged_sentences for _, tag in sentence)\n",
        "\n",
        "# Initialize probabilities\n",
        "transition_probs = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "emission_probs = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "initial_probs = defaultdict(lambda: 0)\n",
        "\n",
        "# Count occurrences for probabilities\n",
        "tag_count = defaultdict(lambda: 0)\n",
        "\n",
        "for sentence in tagged_sentences:\n",
        "    prev_tag = None\n",
        "    for i, (word, tag) in enumerate(sentence):\n",
        "        tag_count[tag] += 1\n",
        "        emission_probs[tag][word] += 1\n",
        "        if i == 0:\n",
        "            initial_probs[tag] += 1\n",
        "        if prev_tag is not None:\n",
        "            transition_probs[prev_tag][tag] += 1\n",
        "        prev_tag = tag\n",
        "\n",
        "# Normalize probabilities\n",
        "total_sentences = len(tagged_sentences)\n",
        "\n",
        "for tag in tagset:\n",
        "    initial_probs[tag] = initial_probs[tag] / total_sentences\n",
        "    total_transitions = sum(transition_probs[tag].values())\n",
        "    total_emissions = sum(emission_probs[tag].values())\n",
        "\n",
        "    for next_tag in tagset:\n",
        "        transition_probs[tag][next_tag] = (transition_probs[tag][next_tag] + 1) / (total_transitions + len(tagset))  # Laplace smoothing\n",
        "\n",
        "    for word in vocab:\n",
        "        emission_probs[tag][word] = (emission_probs[tag][word] + 1) / (total_emissions + len(vocab))  # Laplace smoothing\n",
        "\n",
        "# Viterbi Algorithm Implementation\n",
        "def viterbi(sentence):\n",
        "    V = [{}]  # Store probabilities\n",
        "    path = {}  # Store best paths\n",
        "\n",
        "    # Initialization step\n",
        "    for tag in tagset:\n",
        "        V[0][tag] = np.log(initial_probs[tag] + 1e-6) + np.log(emission_probs[tag].get(sentence[0], 1e-6))\n",
        "        path[tag] = [tag]\n",
        "\n",
        "    # Recursion step\n",
        "    for t in range(1, len(sentence)):\n",
        "        V.append({})\n",
        "        new_path = {}\n",
        "\n",
        "        for tag in tagset:\n",
        "            (prob, state) = max(\n",
        "                (V[t-1][prev_tag] + np.log(transition_probs[prev_tag].get(tag, 1e-6)) +\n",
        "                 np.log(emission_probs[tag].get(sentence[t], 1e-6)), prev_tag) for prev_tag in tagset)\n",
        "            V[t][tag] = prob\n",
        "            new_path[tag] = path[state] + [tag]\n",
        "\n",
        "        path = new_path\n",
        "\n",
        "    # Termination step\n",
        "    (prob, state) = max((V[len(sentence) - 1][tag], tag) for tag in tagset)\n",
        "    return path[state]\n",
        "\n",
        "# Take user input and tokenize\n",
        "user_sentence = input(\"Enter a sentence: \").lower()\n",
        "user_tokens = nltk.word_tokenize(user_sentence)\n",
        "\n",
        "# Predict POS tags\n",
        "predicted_tags = viterbi(user_tokens)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nPredicted POS Tags:\")\n",
        "for word, tag in zip(user_tokens, predicted_tags):\n",
        "    print(f\"{word} -> {tag}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrJT3fZbQriOPlebbqylB2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}