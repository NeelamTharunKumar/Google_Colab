{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgMqHY2u6eZ3rCEsbblRqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeelamTharunKumar/Google_Colab/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import math\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
        "    return tokens\n",
        "\n",
        "# Function to generate n-grams\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "\n",
        "# Function to train n-gram model\n",
        "def train_ngram_model(tokens, n):\n",
        "    return Counter(generate_ngrams(tokens, n))\n",
        "\n",
        "# Laplace smoothing function\n",
        "def laplace_smoothing(ngram_counts, n_minus1_counts, vocab_size, ngram):\n",
        "    return (ngram_counts[ngram] + 1) / (n_minus1_counts[ngram[:-1]] + vocab_size)\n",
        "\n",
        "# Compute perplexity\n",
        "def compute_perplexity(test_sentence, ngram_counts, n_minus1_counts, vocab_size, n):\n",
        "    test_tokens = preprocess_text(test_sentence)\n",
        "    test_ngrams = generate_ngrams(test_tokens, n)\n",
        "    log_prob_sum = 0\n",
        "    for ngram in test_ngrams:\n",
        "        prob = laplace_smoothing(ngram_counts, n_minus1_counts, vocab_size, ngram)\n",
        "        log_prob_sum += math.log(prob)\n",
        "    perplexity = math.exp(-log_prob_sum / len(test_ngrams))\n",
        "    return perplexity\n",
        "\n",
        "# Sample corpus\n",
        "corpus = \"\"\"\n",
        "    Natural language processing is a subfield of artificial intelligence.\n",
        "    It enables computers to understand human language.\n",
        "    Language models are essential in NLP tasks.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocessing corpus\n",
        "tokens = preprocess_text(corpus)\n",
        "vocab = set(tokens)\n",
        "vocab_size = len(vocab) + 1  # For smoothing\n",
        "\n",
        "# Train models\n",
        "unigram_counts = train_ngram_model(tokens, 1)\n",
        "bigram_counts = train_ngram_model(tokens, 2)\n",
        "trigram_counts = train_ngram_model(tokens, 3)\n",
        "\n",
        "# Compute n-1 gram counts\n",
        "unigram_context_counts = Counter([ug[0] for ug in generate_ngrams(tokens, 1)])\n",
        "bigram_context_counts = Counter([bg[0] for bg in generate_ngrams(tokens, 1)])\n",
        "trigram_context_counts = Counter([tg[:2] for tg in generate_ngrams(tokens, 2)])\n",
        "\n",
        "# User input for test sentence\n",
        "test_sentence = input(\"Enter a sentence: \")\n",
        "print(\"Perplexity for Unigram Model:\", compute_perplexity(test_sentence, unigram_counts, unigram_context_counts, vocab_size, 1))\n",
        "print(\"Perplexity for Bigram Model:\", compute_perplexity(test_sentence, bigram_counts, bigram_context_counts, vocab_size, 2))\n",
        "print(\"Perplexity for Trigram Model:\", compute_perplexity(test_sentence, trigram_counts, trigram_context_counts, vocab_size, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "R2LPL-O-a5dX",
        "outputId": "653136d9-ab32-4c4c-89f3-c184b94af289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-1-a87ce38345bc>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a87ce38345bc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    01.import nltk\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1daziKg7qAUD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "376d67e3-be50-4bd6-d002-1301311517ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "class LaplaceSmoothing:\n",
        "    def __init__(self, corpus):\n",
        "        self.unigrams = defaultdict(int)\n",
        "        self.bigrams = defaultdict(int)\n",
        "        self.total_unigrams = 0\n",
        "        self.vocab_size = 0\n",
        "\n",
        "        self.train(corpus)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        vocab = set()\n",
        "        for sentence in corpus:\n",
        "            tokens = sentence.split()\n",
        "            vocab.update(tokens)\n",
        "            for i in range(len(tokens)):\n",
        "                self.unigrams[tokens[i]] += 1\n",
        "                self.total_unigrams += 1\n",
        "                if i > 0:\n",
        "                    self.bigrams[(tokens[i-1], tokens[i])] += 1\n",
        "\n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "    def bigram_prob(self, word1, word2):\n",
        "        return (self.bigrams[(word1, word2)] + 1) / (self.unigrams[word1] + self.vocab_size)\n",
        "\n",
        "corpus = [\"the cat sat on the mat\", \"the dog sat on the mat\"]\n",
        "model = LaplaceSmoothing(corpus)\n",
        "\n",
        "print(f\"P(cat | the): {model.bigram_prob('the', 'cat'):.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzrF8g5Sa5aA",
        "outputId": "2cb6c683-2d54-4109-f1f2-061de7f5f22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(cat | the): 0.200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3HU6CLCaxNX",
        "outputId": "f8e588c6-bcdb-41f3-d526-ee23dc2e3cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "class NGramModel:\n",
        "    def __init__(self, n, smoothing=0.001): # Changed _init_ to __init__\n",
        "        \"\"\"\n",
        "        Initialize the N-Gram model.\n",
        "        :param n: Order of the n-gram (1 for unigram, 2 for bigram, etc.)\n",
        "        :param smoothing: Smoothing factor (Lidstone Smoothing)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.model = defaultdict(lambda: 0)  # Stores n-gram counts\n",
        "        self.context_counts = defaultdict(lambda: 0)  # Stores (N-1)-gram counts\n",
        "        self.smoothing = smoothing  # Lidstone smoothing factor\n",
        "        self.vocab_size = 0  # Vocabulary size\n",
        "\n",
        "    def tokenize_and_pad(self, corpus):\n",
        "        \"\"\"\n",
        "        Tokenizes and adds padding to the corpus.\n",
        "        :param corpus: Input text corpus\n",
        "        :return: Tokenized and padded n-grams list\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(corpus.lower())  # Convert text to lowercase and tokenize\n",
        "        self.vocab_size = len(set(tokens))  # Count unique words (Vocabulary size)\n",
        "\n",
        "        # Generate n-grams with padding\n",
        "        ngrams_list = list(ngrams(tokens, self.n, pad_left=True, pad_right=True,\n",
        "                                  left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "        return ngrams_list\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\"\n",
        "        Train the N-Gram model on a given text corpus.\n",
        "        :param corpus: Input text corpus\n",
        "        \"\"\"\n",
        "        ngrams_list = self.tokenize_and_pad(corpus)  # Tokenize and pad the corpus\n",
        "\n",
        "        # Count N-grams\n",
        "        fdist = FreqDist(ngrams_list)\n",
        "        self.model = fdist  # Store n-gram counts\n",
        "\n",
        "        # Count (N-1)-grams for probability calculation\n",
        "        for ngram in ngrams_list:\n",
        "            context = ngram[:-1]  # Extract (N-1) prefix\n",
        "            self.context_counts[context] += 1  # Count occurrences\n",
        "\n",
        "    def raw_probability(self, ngram):\n",
        "        \"\"\"\n",
        "        Compute raw probability without smoothing.\n",
        "        :param ngram: The n-gram tuple\n",
        "        :return: Raw probability\n",
        "        \"\"\"\n",
        "        count_ngram = self.model[ngram]  # Count of the full N-gram\n",
        "        context = ngram[:-1]  # Extract (N-1)-gram\n",
        "        count_context = self.context_counts[context]  # Count of (N-1)-gram\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if count_context == 0:\n",
        "            return 0\n",
        "        return count_ngram / count_context\n",
        "\n",
        "    def smoothed_probability(self, ngram):\n",
        "        \"\"\"\n",
        "        Compute probability with Lidstone smoothing.\n",
        "        :param ngram: The n-gram tuple\n",
        "        :return: Smoothed probability\n",
        "        \"\"\"\n",
        "        count_ngram = self.model[ngram]  # Count of the full N-gram\n",
        "        context = ngram[:-1]  # Extract (N-1)-gram\n",
        "        count_context = self.context_counts[context]  # Count of (N-1)-gram\n",
        "\n",
        "        # Apply Lidstone Smoothing\n",
        "        smoothed_prob = (count_ngram + self.smoothing) / (count_context + self.vocab_size * self.smoothing)\n",
        "        return smoothed_prob\n",
        "\n",
        "    def calculate_perplexity(self, test_sentence):\n",
        "        \"\"\"\n",
        "        Compute perplexity of a test sentence.\n",
        "        :param test_sentence: Input test sentence\n",
        "        :return: Perplexity value\n",
        "        \"\"\"\n",
        "        ngrams_list = self.tokenize_and_pad(test_sentence)  # Tokenize and pad test sentence\n",
        "\n",
        "        log_prob_sum = 0\n",
        "        for ngram in ngrams_list:\n",
        "            prob_before = self.raw_probability(ngram)  # Raw probability (before smoothing)\n",
        "            prob_after = self.smoothed_probability(ngram)  # Smoothed probability\n",
        "\n",
        "            print(f\"N-gram: {ngram}, Raw Prob: {prob_before}, Smoothed Prob: {prob_after}\")  # Debug Info\n",
        "\n",
        "            log_prob_sum += math.log2(prob_after)  # Sum log probabilities\n",
        "\n",
        "        N = len(ngrams_list)  # Total number of n-grams in test sentence\n",
        "        perplexity = 2 ** (-log_prob_sum / N)  # Compute perplexity\n",
        "        return perplexity\n",
        "\n",
        "# Example usage\n",
        "corpus = \"This is a test sentence. This is another test sentence.\"\n",
        "test_sentence = \"This is a test.\"\n",
        "\n",
        "# Unigram Model\n",
        "print(\"\\n---- Unigram Model ----\")\n",
        "unigram_model = NGramModel(n=1)\n",
        "unigram_model.train(corpus)\n",
        "print(\"Unigram Perplexity:\", unigram_model.calculate_perplexity(test_sentence))\n",
        "\n",
        "# Bigram Model\n",
        "print(\"\\n---- Bigram Model ----\")\n",
        "bigram_model = NGramModel(n=2)\n",
        "bigram_model.train(corpus)\n",
        "print(\"Bigram Perplexity:\", bigram_model.calculate_perplexity(test_sentence))\n",
        "\n",
        "# Trigram Model\n",
        "print(\"\\n---- Trigram Model ----\")\n",
        "trigram_model = NGramModel(n=3)\n",
        "trigram_model.train(corpus)\n",
        "print(\"Trigram Perplexity:\", trigram_model.calculate_perplexity(test_sentence))"
      ],
      "metadata": {
        "id": "5MkzeOopbTwQ",
        "outputId": "9c1dce6e-fc23-453a-d2d2-f8b2e2de65fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Unigram Model ----\n",
            "N-gram: ('this',), Raw Prob: 0.16666666666666666, Smoothed Prob: 0.16668054977092875\n",
            "N-gram: ('is',), Raw Prob: 0.16666666666666666, Smoothed Prob: 0.16668054977092875\n",
            "N-gram: ('a',), Raw Prob: 0.08333333333333333, Smoothed Prob: 0.08338192419825072\n",
            "N-gram: ('test',), Raw Prob: 0.16666666666666666, Smoothed Prob: 0.16668054977092875\n",
            "N-gram: ('.',), Raw Prob: 0.16666666666666666, Smoothed Prob: 0.16668054977092875\n",
            "Unigram Perplexity: 6.890927457103827\n",
            "\n",
            "---- Bigram Model ----\n",
            "N-gram: ('<s>', 'this'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('this', 'is'), Raw Prob: 1.0, Smoothed Prob: 0.9980049875311721\n",
            "N-gram: ('is', 'a'), Raw Prob: 0.5, Smoothed Prob: 0.4992518703241895\n",
            "N-gram: ('a', 'test'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('test', '.'), Raw Prob: 0.0, Smoothed Prob: 0.0004987531172069826\n",
            "N-gram: ('.', '</s>'), Raw Prob: 0.5, Smoothed Prob: 0.4992518703241895\n",
            "Bigram Perplexity: 4.4836775498499035\n",
            "\n",
            "---- Trigram Model ----\n",
            "N-gram: ('<s>', '<s>', 'this'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('<s>', 'this', 'is'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('this', 'is', 'a'), Raw Prob: 0.5, Smoothed Prob: 0.4992518703241895\n",
            "N-gram: ('is', 'a', 'test'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('a', 'test', '.'), Raw Prob: 0.0, Smoothed Prob: 0.0009950248756218907\n",
            "N-gram: ('test', '.', '</s>'), Raw Prob: 0, Smoothed Prob: 0.2\n",
            "N-gram: ('.', '</s>', '</s>'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "Trigram Perplexity: 3.7395609380622976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"tharunkumarneelam@gmail.com\"\n",
        "!git config --global user.name \"NeelamTharunKumar\"\n"
      ],
      "metadata": {
        "id": "IRsthI0UbiOK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zsh8aAKfbiQ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}